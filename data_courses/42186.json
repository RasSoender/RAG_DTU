{
    "course title": "42186 Model-based machine learning",
    "Danish title": "Model-based machine learning",
    "Language of instruction": "English",
    "Point( ECTS )": "5",
    "Course type": "MSc\nOffered as a single course\nGeneral competence course (MSc), Business Analytics\nProgramme specific course (MSc), see more\n\n\n\n\nTechnological specialization course (MSc), see more",
    "Schedule": "Spring F5B (Wed 13-17)",
    "Location": "Campus Lyngby",
    "Scope and form": "Lectures and practical laboratories with iPython notebook",
    "Duration of Course": "13 weeks",
    "Type of assessment": "Evaluation of experiments and reports\nThe evaluation consists of two tests (25% of the grade each), and one group project (group size of 4) with individualized report (50% of the grade).",
    "Exam duration": "Each of the two tests has a duration of 1 hour",
    "Aid": "Written works of reference are permitted",
    "Evaluation": "7 step scale , internal examiner",
    "Academic prerequisites": "02806/02450/02807/42184/02402/02403 , Fundamental statistics and probability (e.g. 42585 Business Analytics); knowledge of computer programming (e.g. Python, R, Matlab, Julia, C++, Java).",
    "Responsible": "Filipe Rodrigues , Lyngby Campus, Building 358, Ph. (+45) 4525 6530 , rodr@dtu.dk",
    "Department": "42 Department of Technology, Management and Economics",
    "Registration Sign up": "At the Studyplanner",
    "General course objectives": "General course objectives\nThis course is designed for engineers, systems analysts, statisticians or related professionals looking to perform advanced data analysis in their future research or practice. Model-based machine learning corresponds to a class of algorithms, called Probabilistic Graphical Models (PGMs), that allow the combination of domain knowledge with data-driven methods, in a very simple way.\n\nWhile Machine Learning has plenty of algorithms (e.g. Neural Networks, Gaussian Processes, Support Vector Machines, Decision Trees, etc.) that have the benefit of being “push-button” solutions, they are generally very hardly adaptable beyond the original design. Our task becomes about transforming our problem and data to fit each individual algorithm. Many times, we drop relevant information (e.g. known relationship between 2 variables, different noise distributions in input variables), and our results may suffer from it.\n\nPGMs allow us to include prior knowledge, parametric and non-parametric (sub)-models, and uncertainty about inputs and parameters. They are perfect for combining different types of data, and, in the past few years, a growing community has developed tools for PGMs, that simplify its design and inference process. Together with Deep Learning, PGMs belong to the forefront of Machine Learning and Data Mining research, essential to process Big and Small Data.\n\nWhile this course is, in nature, about methodology, it is grounded on a sequence of example applications, mostly focused on transport system problems.",
    "Learning objectives": "A student who has met the objectives of the course will be able to:\nExplain central concepts in model-based machine learning, including probabilistic graphical models (PGMs), Bayesian inference and belief propagation\nExamine use cases for different PGMs and distinguish their underlying assumptions\nImplement PGMs in a probabilistic programming language (e.g. Pyro or Stan)\nRecognise practical data modelling aspects, like overfitting, system (e.g. spatial-temporal) dynamics, conditional independence, imputation, conjugate prior\nEvaluate quality of different models for given a problem and dataset\nRelate existing problems and data with modelling approaches to tackle them\nFormulate new models given a problem and data\nDevelop and present a project based on a PGM\nPresent, and be able to argue for, a project based on a PGM",
    "Content": "This course will have lectures with slides and with laboratory work using interactive tools (Jupyter notebooks in Python using a probabilistic programming language like Pyro or STAN). Students will always work manually each module, during and after the theoretical class, to assimilate new concepts. It is designed to be incremental, and strongly supported with practice.\n\nModules:\n- Review of basics – random variable, probability distributions, Bayes Theorem\n- Probabilistic Graphical Models foundations – Bayesian networks, factorization,\nD-separation, conditional independence\n- Probabilistic Graphical Models – Generative models, Representing your own problem\n- Different models – Regression, Classification, Hierarchical Models, Temporal models, Generative models, Gaussian Processes\n- Inference – Exact Inference\n- Inference – Markov Chain Monte Carlo\n- Inference – Variational Inference\n- Advanced topics",
    "Course literature": "\"Model Based Machine Learning\", John Winn, Christopher Bishop, Thomas Diethe, http://www.mbmlbook.com\nExcerpts from (list may be extended):\n\"Pattern Recognition and Machine Learning\", Christopher Bishop\n\"Probabilistic Graphical Models\", Daphne Koller and Nir Friedman",
    "Last updated": "05. februar, 2025"
}